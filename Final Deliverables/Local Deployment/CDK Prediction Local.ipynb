#************************************************* MODEL CREATION **************************************************************
#importing necessary packages
import pandas as pd
import numpy as np
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
#loaing datset
df=pd.read_csv('chronickidneydisease.csv')
df.head()
id	age	bp	sg	al	su	rbc	pc	pcc	ba	...	pcv	wc	rc	htn	dm	cad	appet	pe	ane	classification
0	0	48.0	80.0	1.020	1.0	0.0	NaN	normal	notpresent	notpresent	...	44	7800	5.2	yes	yes	no	good	no	no	ckd
1	1	7.0	50.0	1.020	4.0	0.0	NaN	normal	notpresent	notpresent	...	38	6000	NaN	no	no	no	good	no	no	ckd
2	2	62.0	80.0	1.010	2.0	3.0	normal	normal	notpresent	notpresent	...	31	7500	NaN	no	yes	no	poor	no	yes	ckd
3	3	48.0	70.0	1.005	4.0	0.0	normal	abnormal	present	notpresent	...	32	6700	3.9	yes	no	no	poor	yes	yes	ckd
4	4	51.0	80.0	1.010	2.0	0.0	normal	normal	notpresent	notpresent	...	35	7300	4.6	no	no	no	good	no	no	ckd
5 rows × 26 columns

#checking size of the dataset
df.shape
(400, 26)
#fetching info about columns in dataset
df.info()
RangeIndex: 400 entries, 0 to 399
Data columns (total 26 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   id              400 non-null    int64  
 1   age             391 non-null    float64
 2   bp              388 non-null    float64
 3   sg              353 non-null    float64
 4   al              354 non-null    float64
 5   su              351 non-null    float64
 6   rbc             248 non-null    object 
 7   pc              335 non-null    object 
 8   pcc             396 non-null    object 
 9   ba              396 non-null    object 
 10  bgr             356 non-null    float64
 11  bu              381 non-null    float64
 12  sc              383 non-null    float64
 13  sod             313 non-null    float64
 14  pot             312 non-null    float64
 15  hemo            348 non-null    float64
 16  pcv             330 non-null    object 
 17  wc              295 non-null    object 
 18  rc              270 non-null    object 
 19  htn             398 non-null    object 
 20  dm              398 non-null    object 
 21  cad             398 non-null    object 
 22  appet           399 non-null    object 
 23  pe              399 non-null    object 
 24  ane             399 non-null    object 
 25  classification  400 non-null    object 
dtypes: float64(11), int64(1), object(14)
memory usage: 81.4+ KB
#checking the value counts
for column in  df.columns:
    print(df[column].value_counts(),end="\n")
0      1
263    1
273    1
272    1
271    1
      ..
130    1
129    1
128    1
127    1
399    1
Name: id, Length: 400, dtype: int64
60.0    19
65.0    17
48.0    12
55.0    12
50.0    12
        ..
83.0     1
27.0     1
14.0     1
81.0     1
79.0     1
Name: age, Length: 76, dtype: int64
80.0     116
70.0     112
60.0      71
90.0      53
100.0     25
50.0       5
110.0      3
140.0      1
180.0      1
120.0      1
Name: bp, dtype: int64
1.020    106
1.010     84
1.025     81
1.015     75
1.005      7
Name: sg, dtype: int64
0.0    199
1.0     44
2.0     43
3.0     43
4.0     24
5.0      1
Name: al, dtype: int64
0.0    290
2.0     18
3.0     14
4.0     13
1.0     13
5.0      3
Name: su, dtype: int64
normal      201
abnormal     47
Name: rbc, dtype: int64
normal      259
abnormal     76
Name: pc, dtype: int64
notpresent    354
present        42
Name: pcc, dtype: int64
notpresent    374
present        22
Name: ba, dtype: int64
99.0     10
93.0      9
100.0     9
107.0     8
131.0     6
         ..
288.0     1
182.0     1
84.0      1
256.0     1
226.0     1
Name: bgr, Length: 146, dtype: int64
46.0     15
25.0     13
19.0     11
40.0     10
50.0      9
         ..
176.0     1
145.0     1
92.0      1
322.0     1
186.0     1
Name: bu, Length: 118, dtype: int64
1.2     40
1.1     24
0.5     23
1.0     23
0.9     22
        ..
3.8      1
12.2     1
9.2      1
13.8     1
0.4      1
Name: sc, Length: 84, dtype: int64
135.0    40
140.0    25
141.0    22
139.0    21
138.0    20
142.0    20
137.0    19
150.0    17
136.0    17
147.0    13
145.0    11
132.0    10
146.0    10
131.0     9
144.0     9
133.0     8
130.0     7
134.0     6
143.0     4
124.0     3
127.0     3
122.0     2
113.0     2
120.0     2
125.0     2
128.0     2
114.0     2
126.0     1
163.0     1
115.0     1
129.0     1
4.5       1
104.0     1
111.0     1
Name: sod, dtype: int64
3.5     30
5.0     30
4.9     27
4.7     17
4.8     16
3.9     14
3.8     14
4.1     14
4.2     14
4.0     14
4.4     14
4.5     13
4.3     12
3.7     12
3.6      8
4.6      7
3.4      5
5.2      5
5.3      4
5.7      4
3.2      3
5.5      3
6.3      3
5.4      3
2.9      3
3.3      3
5.6      2
3.0      2
6.5      2
2.5      2
5.9      2
5.8      2
7.6      1
47.0     1
6.6      1
5.1      1
6.4      1
2.8      1
2.7      1
39.0     1
Name: pot, dtype: int64
15.0    16
10.9     8
13.6     7
13.0     7
9.8      7
        ..
6.8      1
8.5      1
7.3      1
12.8     1
17.6     1
Name: hemo, Length: 115, dtype: int64
41      21
52      21
44      19
48      19
40      16
43      14
42      13
45      13
32      12
36      12
33      12
50      12
28      12
34      11
37      11
30       9
29       9
35       9
46       9
31       8
24       7
39       7
26       6
38       5
53       4
51       4
49       4
47       4
54       4
25       3
27       3
22       3
19       2
23       2
15       1
21       1
17       1
20       1
\t43     1
18       1
9        1
14       1
\t?      1
16       1
Name: pcv, dtype: int64
9800     11
6700     10
9200      9
9600      9
7200      9
         ..
19100     1
\t?       1
12300     1
14900     1
12700     1
Name: wc, Length: 92, dtype: int64
5.2    18
4.5    16
4.9    14
4.7    11
4.8    10
3.9    10
4.6     9
3.4     9
5.9     8
5.5     8
6.1     8
5.0     8
3.7     8
5.3     7
5.8     7
5.4     7
3.8     7
5.6     6
4.3     6
4.2     6
3.2     5
4.4     5
5.7     5
6.4     5
5.1     5
6.2     5
6.5     5
4.1     5
3.6     4
6.3     4
6.0     4
4.0     3
3.3     3
4       3
3.5     3
2.9     2
3.1     2
2.6     2
2.1     2
2.5     2
2.8     2
3.0     2
2.7     2
5       2
2.3     1
\t?     1
2.4     1
3       1
8.0     1
Name: rc, dtype: int64
no     251
yes    147
Name: htn, dtype: int64
no       258
yes      134
\tno       3
\tyes      2
 yes       1
Name: dm, dtype: int64
no      362
yes      34
\tno      2
Name: cad, dtype: int64
good    317
poor     82
Name: appet, dtype: int64
no     323
yes     76
Name: pe, dtype: int64
no     339
yes     60
Name: ane, dtype: int64
ckd       248
notckd    150
ckd\t       2
Name: classification, dtype: int64
#refactoring the dataset by replacing valid data
df.replace(to_replace="\tno",
           value="no",inplace=True)
df.replace(to_replace="\tyes",
           value="yes",inplace=True)
df.replace(to_replace=" yes",
           value="yes",inplace=True)
df.replace(to_replace="ckd\t",
           value="ckd",inplace=True)
df.replace(to_replace="\t43",
           value="43",inplace=True)
df.replace({'\t?': None},inplace=True)
df.replace(to_replace="\t6200",
           value="6200",inplace=True)
df.replace(to_replace="\t8400",
           value="8400",inplace=True)
#checking the value counts after refactoring 
for column in  df.columns:
    print(df[column].value_counts(),end="\n")
0      1
263    1
273    1
272    1
271    1
      ..
130    1
129    1
128    1
127    1
399    1
Name: id, Length: 400, dtype: int64
60.0    19
65.0    17
48.0    12
55.0    12
50.0    12
        ..
83.0     1
27.0     1
14.0     1
81.0     1
79.0     1
Name: age, Length: 76, dtype: int64
80.0     116
70.0     112
60.0      71
90.0      53
100.0     25
50.0       5
110.0      3
140.0      1
180.0      1
120.0      1
Name: bp, dtype: int64
1.020    106
1.010     84
1.025     81
1.015     75
1.005      7
Name: sg, dtype: int64
0.0    199
1.0     44
2.0     43
3.0     43
4.0     24
5.0      1
Name: al, dtype: int64
0.0    290
2.0     18
3.0     14
4.0     13
1.0     13
5.0      3
Name: su, dtype: int64
normal      201
abnormal     47
Name: rbc, dtype: int64
normal      259
abnormal     76
Name: pc, dtype: int64
notpresent    354
present        42
Name: pcc, dtype: int64
notpresent    374
present        22
Name: ba, dtype: int64
99.0     10
93.0      9
100.0     9
107.0     8
131.0     6
         ..
288.0     1
182.0     1
84.0      1
256.0     1
226.0     1
Name: bgr, Length: 146, dtype: int64
46.0     15
25.0     13
19.0     11
40.0     10
50.0      9
         ..
176.0     1
145.0     1
92.0      1
322.0     1
186.0     1
Name: bu, Length: 118, dtype: int64
1.2     40
1.1     24
0.5     23
1.0     23
0.9     22
        ..
3.8      1
12.2     1
9.2      1
13.8     1
0.4      1
Name: sc, Length: 84, dtype: int64
135.0    40
140.0    25
141.0    22
139.0    21
138.0    20
142.0    20
137.0    19
150.0    17
136.0    17
147.0    13
145.0    11
132.0    10
146.0    10
131.0     9
144.0     9
133.0     8
130.0     7
134.0     6
143.0     4
124.0     3
127.0     3
122.0     2
113.0     2
120.0     2
125.0     2
128.0     2
114.0     2
126.0     1
163.0     1
115.0     1
129.0     1
4.5       1
104.0     1
111.0     1
Name: sod, dtype: int64
3.5     30
5.0     30
4.9     27
4.7     17
4.8     16
3.9     14
3.8     14
4.1     14
4.2     14
4.0     14
4.4     14
4.5     13
4.3     12
3.7     12
3.6      8
4.6      7
3.4      5
5.2      5
5.3      4
5.7      4
3.2      3
5.5      3
6.3      3
5.4      3
2.9      3
3.3      3
5.6      2
3.0      2
6.5      2
2.5      2
5.9      2
5.8      2
7.6      1
47.0     1
6.6      1
5.1      1
6.4      1
2.8      1
2.7      1
39.0     1
Name: pot, dtype: int64
15.0    16
10.9     8
13.6     7
13.0     7
9.8      7
        ..
6.8      1
8.5      1
7.3      1
12.8     1
17.6     1
Name: hemo, Length: 115, dtype: int64
41    21
52    21
44    19
48    19
40    16
43    15
42    13
45    13
32    12
50    12
36    12
33    12
28    12
34    11
37    11
30     9
29     9
35     9
46     9
31     8
24     7
39     7
26     6
38     5
53     4
51     4
49     4
47     4
54     4
25     3
27     3
22     3
19     2
23     2
15     1
21     1
20     1
17     1
9      1
18     1
14     1
16     1
Name: pcv, dtype: int64
9800     11
6700     10
9600      9
7200      9
9200      9
         ..
19100     1
12300     1
16700     1
14900     1
2600      1
Name: wc, Length: 89, dtype: int64
5.2    18
4.5    16
4.9    14
4.7    11
4.8    10
3.9    10
4.6     9
3.4     9
5.9     8
5.5     8
6.1     8
5.0     8
3.7     8
5.3     7
5.8     7
5.4     7
3.8     7
5.6     6
4.3     6
4.2     6
3.2     5
4.4     5
5.7     5
6.4     5
5.1     5
6.2     5
6.5     5
4.1     5
3.6     4
6.0     4
6.3     4
4.0     3
3.5     3
3.3     3
4       3
5       2
3.1     2
2.6     2
2.1     2
2.9     2
2.5     2
3.0     2
2.7     2
2.8     2
2.3     1
2.4     1
3       1
8.0     1
Name: rc, dtype: int64
no     251
yes    147
Name: htn, dtype: int64
no     261
yes    137
Name: dm, dtype: int64
no     364
yes     34
Name: cad, dtype: int64
good    317
poor     82
Name: appet, dtype: int64
no     323
yes     76
Name: pe, dtype: int64
no     339
yes     60
Name: ane, dtype: int64
ckd       250
notckd    150
Name: classification, dtype: int64
#checking if there is any null values in descending order
df.isnull().sum().sort_values(ascending=False)
rbc               152
rc                131
wc                106
pot                88
sod                87
pcv                71
pc                 65
hemo               52
su                 49
sg                 47
al                 46
bgr                44
bu                 19
sc                 17
bp                 12
age                 9
ba                  4
pcc                 4
htn                 2
dm                  2
cad                 2
ane                 1
appet               1
pe                  1
id                  0
classification      0
dtype: int64
#converting the misinterpreted data types to valid one
df['pcv']=df['pcv'].astype('float64')
df['wc']=df['wc'].astype('float64')
df['rc']=df['rc'].astype('float64')
#checking the info to see the changes in data types
df.info()
RangeIndex: 400 entries, 0 to 399
Data columns (total 26 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   id              400 non-null    int64  
 1   age             391 non-null    float64
 2   bp              388 non-null    float64
 3   sg              353 non-null    float64
 4   al              354 non-null    float64
 5   su              351 non-null    float64
 6   rbc             248 non-null    object 
 7   pc              335 non-null    object 
 8   pcc             396 non-null    object 
 9   ba              396 non-null    object 
 10  bgr             356 non-null    float64
 11  bu              381 non-null    float64
 12  sc              383 non-null    float64
 13  sod             313 non-null    float64
 14  pot             312 non-null    float64
 15  hemo            348 non-null    float64
 16  pcv             329 non-null    float64
 17  wc              294 non-null    float64
 18  rc              269 non-null    float64
 19  htn             398 non-null    object 
 20  dm              398 non-null    object 
 21  cad             398 non-null    object 
 22  appet           399 non-null    object 
 23  pe              399 non-null    object 
 24  ane             399 non-null    object 
 25  classification  400 non-null    object 
dtypes: float64(14), int64(1), object(11)
memory usage: 81.4+ KB
#replacing null values with modes for categorical column
#and replacing null values with median for nominal column 

df['age'].fillna(df['age'].median(),inplace=True)
df['bp'].fillna(df['bp'].median(),inplace=True)
df['sg'].fillna(df['sg'].median(),inplace=True)
df['al'].fillna(df['al'].median(),inplace=True)
df['su'].fillna(df['su'].median(),inplace=True)
df['rbc'].fillna(df['rbc'].mode()[0],inplace=True)
df['pc'].fillna(df['pc'].mode()[0],inplace=True)
df['pcc'].fillna(df['pcc'].mode()[0],inplace=True)
df['ba'].fillna(df['ba'].mode()[0],inplace=True)
df['bgr'].fillna(df['bgr'].median(),inplace=True)
df['bu'].fillna(df['bu'].median(),inplace=True)
df['sc'].fillna(df['sc'].median(),inplace=True)
df['sod'].fillna(df['sod'].median(),inplace=True)
df['pot'].fillna(df['pot'].median(),inplace=True)
df['hemo'].fillna(df['hemo'].median(),inplace=True)
df['pcv'].fillna(df['pcv'].median(),inplace=True)
df['wc'].fillna(df['wc'].median(),inplace=True)
df['rc'].fillna(df['rc'].median(),inplace=True)
df['htn'].fillna(df['htn'].mode()[0],inplace=True)
df['dm'].fillna(df['dm'].mode()[0],inplace=True)
df['cad'].fillna(df['cad'].mode()[0],inplace=True)
df['appet'].fillna(df['appet'].mode()[0],inplace=True)
df['pe'].fillna(df['pe'].mode()[0],inplace=True)
df['ane'].fillna(df['ane'].mode()[0],inplace=True)
df.isnull().sum() #checking if there is any null values present
id                0
age               0
bp                0
sg                0
al                0
su                0
rbc               0
pc                0
pcc               0
ba                0
bgr               0
bu                0
sc                0
sod               0
pot               0
hemo              0
pcv               0
wc                0
rc                0
htn               0
dm                0
cad               0
appet             0
pe                0
ane               0
classification    0
dtype: int64
#finding out numerical column filtering out from categorical column

num_cols=list(df.select_dtypes(['float64']))
num_cols
['age',
 'bp',
 'sg',
 'al',
 'su',
 'bgr',
 'bu',
 'sc',
 'sod',
 'pot',
 'hemo',
 'pcv',
 'wc',
 'rc']
#describing the dataset 
df.describe()
id	age	bp	sg	al	su	bgr	bu	sc	sod	pot	hemo	pcv	wc	rc
count	400.000000	400.000000	400.000000	400.000000	400.00000	400.000000	400.000000	400.000000	400.000000	400.000000	400.000000	400.00000	400.000000	400.000000	400.000000
mean	199.500000	51.562500	76.575000	1.017712	0.90000	0.395000	145.062500	56.693000	2.997125	137.631250	4.577250	12.54250	39.082500	8298.500000	4.737750
std	115.614301	16.982996	13.489785	0.005434	1.31313	1.040038	75.260774	49.395258	5.628886	9.206332	2.821357	2.71649	8.162245	2529.593814	0.841439
min	0.000000	2.000000	50.000000	1.005000	0.00000	0.000000	22.000000	1.500000	0.400000	4.500000	2.500000	3.10000	9.000000	2200.000000	2.100000
25%	99.750000	42.000000	70.000000	1.015000	0.00000	0.000000	101.000000	27.000000	0.900000	135.000000	4.000000	10.87500	34.000000	6975.000000	4.500000
50%	199.500000	55.000000	80.000000	1.020000	0.00000	0.000000	121.000000	42.000000	1.300000	138.000000	4.400000	12.65000	40.000000	8000.000000	4.800000
75%	299.250000	64.000000	80.000000	1.020000	2.00000	0.000000	150.000000	61.750000	2.725000	141.000000	4.800000	14.62500	44.000000	9400.000000	5.100000
max	399.000000	90.000000	180.000000	1.025000	5.00000	5.000000	490.000000	391.000000	76.000000	163.000000	47.000000	17.80000	54.000000	26400.000000	8.000000
#univariate analysis

import matplotlib.pyplot as plt
for col in df[['rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane','classification']]:
      fig= plt.subplots(1,1, figsize = (13,7))
      sns.countplot(x=df[col])











import matplotlib.pyplot as plt
for col in num_cols:
      fig= plt.subplots(1,1, figsize = (13,7))
      sns.distplot(x=df[col])














# outlier detection
# detecting outliers if is it present through boxplot

import matplotlib.pyplot as plt
for col in num_cols:
  fig= plt.subplots(1,1, figsize = (13,7))
  sns.boxplot(df[col], orient = 'v')
 














#bivariate analysis
sns.lineplot(df['age'],df['bp'])

sns.scatterplot(df['age'],df['bgr'],hue=df["classification"])

sns.scatterplot(df['bp'],df['bgr'],hue=df["classification"])

sns.scatterplot(df['wc'],df['bu'],hue=df["classification"])

#multivariate analysis
fig, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(df.corr(),annot=True,ax=ax) #there is not multicolinearity

sns.pairplot(df,hue='classification')

#replacing the outliers with IQR median method

def removeOutlier(cols):
    q1=df[cols].quantile(0.25)
    q3=df[cols].quantile(0.75)
    iqr=q3-q1
    upperlimit=q3+1.5*iqr
    lowerlimit=q1-1.5*iqr
    df[cols]=np.where(df[cols]>upperlimit,df[cols].median(),df[cols])
    df[cols]=np.where(df[cols]<lowerlimit,df[cols].median(),df[cols])
    print('***************'+cols+'********************')
    print(q1,q3,iqr)
    

for cols in num_cols:
    removeOutlier(cols);
***************age********************
42.0 64.0 22.0
***************bp********************
70.0 80.0 10.0
***************sg********************
1.015 1.02 0.0050000000000001155
***************al********************
0.0 2.0 2.0
***************su********************
0.0 0.0 0.0
***************bgr********************
101.0 150.0 49.0
***************bu********************
27.0 61.75 34.75
***************sc********************
0.9 2.725 1.8250000000000002
***************sod********************
135.0 141.0 6.0
***************pot********************
4.0 4.8 0.7999999999999998
***************hemo********************
10.875 14.625 3.75
***************pcv********************
34.0 44.0 10.0
***************wc********************
6975.0 9400.0 2425.0
***************rc********************
4.5 5.1 0.5999999999999996
#checking the outliers once again with boxplot 

for col in num_cols:
  fig= plt.subplots(1,1, figsize = (13,7))
  sns.boxplot(df[col])














#dropping unwanted column which doesnt make any effect on prediction

df.drop(['id'],axis=1,inplace=True)
df.head()
age	bp	sg	al	su	rbc	pc	pcc	ba	bgr	...	pcv	wc	rc	htn	dm	cad	appet	pe	ane	classification
0	48.0	80.0	1.02	1.0	0.0	normal	normal	notpresent	notpresent	121.0	...	44.0	7800.0	5.2	yes	yes	no	good	no	no	ckd
1	55.0	80.0	1.02	4.0	0.0	normal	normal	notpresent	notpresent	121.0	...	38.0	6000.0	4.8	no	no	no	good	no	no	ckd
2	62.0	80.0	1.01	2.0	0.0	normal	normal	notpresent	notpresent	121.0	...	31.0	7500.0	4.8	no	yes	no	poor	no	yes	ckd
3	48.0	70.0	1.02	4.0	0.0	normal	abnormal	present	notpresent	117.0	...	32.0	6700.0	3.9	yes	no	no	poor	yes	yes	ckd
4	51.0	80.0	1.01	2.0	0.0	normal	normal	notpresent	notpresent	106.0	...	35.0	7300.0	4.6	no	no	no	good	no	no	ckd
5 rows × 25 columns

#label encoding for categorical attributes

df['rbc']=df['rbc'].map({'normal':0,'abnormal':1})
df['pc']=df['pc'].map({'normal':0,'abnormal':1})
df['pcc']=df['pcc'].map({'notpresent':0,'present':1})
df['ba']=df['ba'].map({'notpresent':0,'present':1})
df['htn']=df['htn'].map({'no':0,'yes':1})
df['dm']=df['dm'].map({'no':0,'yes':1})
df['cad']=df['cad'].map({'no':0,'yes':1})
df['pe']=df['pe'].map({'no':0,'yes':1})
df['ane']=df['ane'].map({'no':0,'yes':1})
df['appet']=df['appet'].map({'good':0,'poor':1})
df['classification']=df['classification'].map({'ckd':1,'notckd':0})
#seperating independent variable

X=df.iloc[:,:-1]
X
age	bp	sg	al	su	rbc	pc	pcc	ba	bgr	...	hemo	pcv	wc	rc	htn	dm	cad	appet	pe	ane
0	48.0	80.0	1.020	1.0	0.0	0	0	0	0	121.0	...	15.4	44.0	7800.0	5.2	1	1	0	0	0	0
1	55.0	80.0	1.020	4.0	0.0	0	0	0	0	121.0	...	11.3	38.0	6000.0	4.8	0	0	0	0	0	0
2	62.0	80.0	1.010	2.0	0.0	0	0	0	0	121.0	...	9.6	31.0	7500.0	4.8	0	1	0	1	0	1
3	48.0	70.0	1.020	4.0	0.0	0	1	1	0	117.0	...	11.2	32.0	6700.0	3.9	1	0	0	1	1	1
4	51.0	80.0	1.010	2.0	0.0	0	0	0	0	106.0	...	11.6	35.0	7300.0	4.6	0	0	0	0	0	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
395	55.0	80.0	1.020	0.0	0.0	0	0	0	0	140.0	...	15.7	47.0	6700.0	4.9	0	0	0	0	0	0
396	42.0	70.0	1.025	0.0	0.0	0	0	0	0	75.0	...	16.5	54.0	7800.0	4.8	0	0	0	0	0	0
397	12.0	80.0	1.020	0.0	0.0	0	0	0	0	100.0	...	15.8	49.0	6600.0	5.4	0	0	0	0	0	0
398	17.0	60.0	1.025	0.0	0.0	0	0	0	0	114.0	...	14.2	51.0	7200.0	5.9	0	0	0	0	0	0
399	58.0	80.0	1.025	0.0	0.0	0	0	0	0	131.0	...	15.8	53.0	6800.0	4.8	0	0	0	0	0	0
400 rows × 24 columns

#seperating dependent variable

y=df.iloc[:,-1]
y
0      1
1      1
2      1
3      1
4      1
      ..
395    0
396    0
397    0
398    0
399    0
Name: classification, Length: 400, dtype: int64
#splitting the data into training and test set for model creation using train test split

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=8)

X_train
age	bp	sg	al	su	rbc	pc	pcc	ba	bgr	...	hemo	pcv	wc	rc	htn	dm	cad	appet	pe	ane
240	65.0	70.0	1.015	1.0	0.0	0	0	0	0	203.0	...	11.40	36.0	5000.0	4.1	1	1	0	1	1	0
274	19.0	80.0	1.020	0.0	0.0	0	0	0	0	107.0	...	14.40	44.0	8000.0	4.8	0	0	0	0	0	0
78	70.0	80.0	1.020	0.0	0.0	0	0	0	0	158.0	...	10.10	30.0	8000.0	4.8	1	0	0	0	1	0
289	42.0	70.0	1.020	0.0	0.0	0	0	0	0	93.0	...	16.60	43.0	7100.0	5.3	0	0	0	0	0	0
73	55.0	80.0	1.015	2.0	0.0	1	1	0	0	129.0	...	12.65	40.0	6300.0	4.8	1	0	0	0	1	1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
339	25.0	70.0	1.020	0.0	0.0	0	0	0	0	88.0	...	13.30	48.0	7000.0	4.9	0	0	0	0	0	0
136	46.0	90.0	1.020	0.0	0.0	0	0	0	0	213.0	...	9.30	40.0	8000.0	4.8	1	1	0	0	0	0
133	70.0	80.0	1.015	4.0	0.0	0	0	0	0	118.0	...	12.00	37.0	8400.0	4.8	1	0	0	0	0	0
361	29.0	80.0	1.020	0.0	0.0	0	0	0	0	70.0	...	13.70	54.0	5400.0	5.8	0	0	0	0	0	0
340	32.0	70.0	1.025	0.0	0.0	0	0	0	0	100.0	...	14.30	43.0	6700.0	5.9	0	0	0	0	0	0
280 rows × 24 columns

#checking the training data shape 

X_train.shape
(280, 24)
#using random forest classifier for first model for prediction

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier()
#fitting the model over training data
model.fit(X_train,y_train)
RandomForestClassifier()
y_pred=model.predict(X_test)
#diplaying predicted data
y_pred
array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
       0, 1, 0, 0, 0, 1, 1, 0, 0, 1], dtype=int64)
#importing metrics to check the performance of the model

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
#displaying accuracy score of testing data
accuracy_score(y_test,y_pred) #higher accuracy
0.9916666666666667
#confusion matrix display
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)

#classification report displaying  the other performance metrics

print(classification_report(y_test,y_pred));
              precision    recall  f1-score   support

           0       1.00      0.98      0.99        42
           1       0.99      1.00      0.99        78

    accuracy                           0.99       120
   macro avg       0.99      0.99      0.99       120
weighted avg       0.99      0.99      0.99       120

# 2nd model is based on Logistic Regression

from sklearn.linear_model import LogisticRegression
model1=LogisticRegression()
model1.fit(X_train,y_train)
LogisticRegression()
y_pred1=model1.predict(X_test)
y_pred1
array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
       0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64)
accuracy_score(y_test,y_pred1) 
0.9166666666666666
sns.heatmap(confusion_matrix(y_test,y_pred1),annot=True)

print(classification_report(y_test,y_pred1))
              precision    recall  f1-score   support

           0       0.90      0.86      0.88        42
           1       0.93      0.95      0.94        78

    accuracy                           0.92       120
   macro avg       0.91      0.90      0.91       120
weighted avg       0.92      0.92      0.92       120

#3rd model based on Naive Bayes Classifier

from sklearn.naive_bayes import GaussianNB
model2=GaussianNB()
model2.fit(X_train,y_train)
GaussianNB()
y_pred3=model2.predict(X_test) 
y_pred3
array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
       0, 1, 0, 0, 0, 1, 1, 0, 0, 1], dtype=int64)
accuracy_score(y_test,y_pred3)
0.975
sns.heatmap(confusion_matrix(y_test,y_pred3),annot=True)

print(classification_report(y_test,y_pred3))
              precision    recall  f1-score   support

           0       0.93      1.00      0.97        42
           1       1.00      0.96      0.98        78

    accuracy                           0.97       120
   macro avg       0.97      0.98      0.97       120
weighted avg       0.98      0.97      0.98       120

#4th model based on Support vector machine 

from sklearn.svm import SVC
model3=SVC()
model3.fit(X_train,y_train)
SVC()
y_pred4=model3.predict(X_test) 
y_pred4
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)
accuracy_score(y_test,y_pred4) #least accuracy
0.65
sns.heatmap(confusion_matrix(y_test,y_pred4),annot=True)

print(classification_report(y_test,y_pred4))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        42
           1       0.65      1.00      0.79        78

    accuracy                           0.65       120
   macro avg       0.33      0.50      0.39       120
weighted avg       0.42      0.65      0.51       120

#5th model is based on Decision tree 

from sklearn.tree import DecisionTreeClassifier
model4=DecisionTreeClassifier(criterion='entropy')
model4.fit(X_train,y_train)
DecisionTreeClassifier(criterion='entropy')
y_pred5=model4.predict(X_test) 
y_pred5
array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
       0, 1, 0, 0, 0, 1, 1, 0, 0, 1], dtype=int64)
accuracy_score(y_test,y_pred5) #has same high accuracy as Random Forest
0.9916666666666667
sns.heatmap(confusion_matrix(y_test,y_pred5),annot=True)

print(classification_report(y_test,y_pred5))
              precision    recall  f1-score   support

           0       0.98      1.00      0.99        42
           1       1.00      0.99      0.99        78

    accuracy                           0.99       120
   macro avg       0.99      0.99      0.99       120
weighted avg       0.99      0.99      0.99       120

Result
Random Forest Classifier and Decision Tree Classifier have accuracy of 99.16% compared to other models in our prediction
Thus we are going to use Random Forest Classifier Model as primary model for chronic kidney disease detection as it solves the problem of overfitting and uses combination of decision tree results
